{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import haliax as hax  # this amuses me more than it should\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# smaller versions of the numbers from the above table\n",
    "Layer = hax.Axis(\"layer\", 4)\n",
    "Head = hax.Axis(\"head\", 8)\n",
    "Key = hax.Axis(\"key\", 16)\n",
    "Embed = hax.Axis(\"embed\", 32)\n",
    "Mlp = hax.Axis(\"mlp\", Embed.size * 4)  # this is the \"feed-forward size\", above\n",
    "\n",
    "# other numbers we need\n",
    "Batch = hax.Axis(\"batch\", 8)\n",
    "Pos = hax.Axis(\"position\", 128)  # how long each sequence is\n",
    "Vocab = hax.Axis(\"vocab\", len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "bias = hax.zeros(Mlp)\n",
    "weight = hax.ones((Embed, Mlp))\n",
    "word_embedding = hax.zeros((Vocab, Embed))\n",
    "data = hax.ones((Batch, Pos), dtype=jnp.int32)\n",
    "layer_indices = hax.arange(Layer)  # 0...Layer.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NamedArray(array=Array([0, 1, 2, 3], dtype=int32), axes=(Axis(name='layer', size=4),))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = jnp.zeros((32, 32 * 4))\n",
    "named_a = hax.named(a, (Embed, Mlp))\n",
    "named_a = hax.named(a, (\"embed\", \"mlp\"))  # ok, b/c axis sizes can be inferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.random\n",
    "from jax.random import PRNGKey\n",
    "\n",
    "base_key = PRNGKey(0)\n",
    "k_w, k_e, k_d = jax.random.split(base_key, 3)  # keys for each of the generations we do below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weight = hax.random.normal(k_w, (Embed, Mlp))\n",
    "word_embedding = hax.random.normal(k_e, (Vocab, Embed))\n",
    "data = hax.random.randint(k_d, (Batch, Pos), 0, Vocab.size)  # samples from [0, Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_weight = hax.mean(weight, Embed) # average each of the 'Embed' rows?\n",
    "m_weight = hax.mean(weight, \"embed\")  # also ok\n",
    "total = hax.sum(weight, (Embed, Mlp))  # equivalent to hax.sum(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NamedArray(array=Array([0, 1, 2, 3, 4], dtype=int32), axes=(Axis(name='M', size=5),))\n",
      "NamedArray(array=Array([0, 1, 2, 3], dtype=int32), axes=(Axis(name='N', size=4),))\n",
      "(Axis(name='N', size=4), Axis(name='M', size=5))\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  1  2  3  4]\n",
      " [ 0  2  4  6  8]\n",
      " [ 0  3  6  9 12]]\n"
     ]
    }
   ],
   "source": [
    "M = hax.Axis(\"M\", 5)\n",
    "N = hax.Axis(\"N\", 4)\n",
    "\n",
    "a = hax.arange(M)\n",
    "b = hax.arange(N)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "c = a.broadcast_axis(N) * b\n",
    "print(c.axes)\n",
    "print(c.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = a.broadcast_axis(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = hax.random.normal(k_w, (Embed, Mlp))\n",
    "word_embedding = hax.random.normal(k_e, (Vocab, Embed)) # (column, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Axis(name='vocab', size=50257), Axis(name='embed', size=32))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding.axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
