{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import GPT2, GPT2Config\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = torch.load('ckpt.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500, Array(1.916147, dtype=float32))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sd['model']\n",
    "sd['step'], sd['best_val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jax.tree_util import tree_flatten\n",
    "\n",
    "import torch\n",
    "import equinox as eqx\n",
    "import haliax as hax\n",
    "import haliax.nn as hnn\n",
    "from haliax import NamedArray\n",
    "\n",
    "from levanter.compat.torch_serialization import (\n",
    "    StateDictSerializationMixin as Serializable,\n",
    "    StateDict,\n",
    "    apply_prefix,\n",
    "    flatten_linear_layers,\n",
    "    stack_state_dict,\n",
    "    unflatten_linear_layers,\n",
    "    unstack_state_dict,\n",
    ")\n",
    "\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPT2Config:\n",
    "    seq_len    : int   = 1024\n",
    "    vocab_size : int   = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    num_layers : int   = 12\n",
    "    num_heads  : int   = 12\n",
    "    hidden_dims: int   = 768\n",
    "    dropout    : float = 0.0\n",
    "    use_bias   : bool  = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.Pos      = hax.Axis(name='position',  size=self.seq_len)\n",
    "        self.Vocab    = hax.Axis(name='vocab',     size=self.vocab_size)\n",
    "        self.Embed    = hax.Axis(name='embed',     size=self.hidden_dims)\n",
    "        self.Heads    = hax.Axis(name='heads',     size=self.num_heads)\n",
    "        self.Layers   = hax.Axis(name='layers',    size=self.num_layers)\n",
    "        self.Mlp      = hax.Axis(name='mlp',       size=self.hidden_dims * 4)\n",
    "        self.HeadSize = hax.Axis(name='head_size', size=self.hidden_dims // self.num_heads)\n",
    "        self.KeyPos   = self.Pos.alias('key_position')\n",
    "\n",
    "    @classmethod\n",
    "    def get_pretrained_config(cls, model_type: str, dropout: float = 0.0) -> 'GPT2Config':\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        print(f'Loading pre-trained {model_type} weights...')\n",
    "\n",
    "        config_args = {\n",
    "            'gpt2'        :  dict(num_layers=12, num_heads=12, hidden_dims=768),  # 124M params\n",
    "            'gpt2-medium' :  dict(num_layers=24, num_heads=16, hidden_dims=1024), # 350M params\n",
    "            'gpt2-large'  :  dict(num_layers=36, num_heads=20, hidden_dims=1280), # 774M params\n",
    "            'gpt2-xl'     :  dict(num_layers=48, num_heads=25, hidden_dims=1600), # 1558M params\n",
    "        }[model_type] | {'vocab_size' : 50257, 'dropout' : dropout}\n",
    "\n",
    "        return GPT2Config(**config_args)\n",
    "\n",
    "\n",
    "class CausalSelfAttention(eqx.Module, Serializable): \n",
    "    attn     : hnn.Linear\n",
    "    proj     : hnn.Linear\n",
    "    dropout  : hnn.Dropout\n",
    "    mask     : NamedArray = eqx.static_field() # not learnable\n",
    "    _scale_f : float      = eqx.static_field() # not learnable\n",
    "\n",
    "    @staticmethod\n",
    "    def init(config: GPT2Config, key) -> 'CausalSelfAttention':\n",
    "        k1, k2   = jr.split(key, 2)\n",
    "        Qkv      = hax.Axis('qkv', size=3) # generate queries, keys, and values all at once\n",
    "\n",
    "        attn = hnn.Linear.init(\n",
    "            In       = config.Embed, \n",
    "            Out      = (Qkv, config.Heads, config.HeadSize),\n",
    "            key      = k1, \n",
    "            use_bias = config.use_bias\n",
    "        )\n",
    "\n",
    "        proj = hnn.Linear.init(\n",
    "            In       = (config.Heads, config.HeadSize), \n",
    "            Out      = config.Embed,\n",
    "            key      = k2,\n",
    "            use_bias = config.use_bias\n",
    "        )\n",
    "\n",
    "        dropout     = hnn.Dropout(config.dropout)\n",
    "        causal_mask = hnn.attention.causal_mask(config.Pos, config.KeyPos)\n",
    "\n",
    "        return CausalSelfAttention(\n",
    "            attn, \n",
    "            proj, \n",
    "            dropout, \n",
    "            causal_mask, \n",
    "            jnp.sqrt(config.HeadSize.size)\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self, \n",
    "        x: NamedArray, \n",
    "        padding_mask: NamedArray = None,\n",
    "        *, \n",
    "        key\n",
    "    ) -> NamedArray:\n",
    "        qkv = self.attn(x).rearrange((..., 'qkv', 'heads', 'position', 'head_size'))\n",
    "        q, k, v, = qkv.unbind('qkv')  # each of q, k, and v is of shape: [Heads, Pos, HeadSize] \n",
    "\n",
    "        # rename `Pos` axis for keys and values (because both axes of q â€¢ k cannot have the same name)\n",
    "        k = k.rename({'position' : 'key_position'})\n",
    "        v = v.rename({'position' : 'key_position'})\n",
    "\n",
    "        scores  = hax.dot('head_size', q, k) # of shape: [..., Heads, Pos, KeyPos]\n",
    "        scores  /= self._scale_f\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            mask = self.mask * padding_mask\n",
    "        else: mask = self.mask\n",
    "\n",
    "        masked_scores      = hax.where(mask, scores, -jnp.inf) # cannot attend to tokens in the future\n",
    "        normalized_scores  = hnn.softmax(masked_scores, axis='key_position')\n",
    "        regularized_scores = self.dropout(normalized_scores, key=key)\n",
    "\n",
    "        x = hax.dot('key_position', regularized_scores, v) # of shape: [..., Heads, Pos, HeadSize]\n",
    "        # ==============================\n",
    "        # multiplying the output of each head by its own matrix and then summing\n",
    "        # is equivalent to first concatenating the output of each head and then\n",
    "        # multiplying by one big matrix, since each region of the concatenated\n",
    "        # vector only interacts with a subset of the overall matrix\n",
    "        # ==============================\n",
    "        x = self.proj(x) # of shape: [..., Pos, Embed]\n",
    "        return x\n",
    "\n",
    "    def _state_dict_key_map(self):\n",
    "        return {'attn' : 'c_attn', 'proj' : 'c_proj'}\n",
    "\n",
    "    def from_state_dict(self, state_dict: StateDict, prefix: Optional[str] = None):\n",
    "        # our c_attn is [embed] -> [3, heads, head_dim] and hf's is the flattened [embed] -> [3 * heads * head_dim]\n",
    "        # and our c_proj is [heads, head_dim] -> [embed] and hf's is the flattened [heads * head_dim] -> [embed]\n",
    "        # so we need to reshape the one in the dict before forwarding to the linear\n",
    "        # keep in mind that everything is vectorized in our implementation, so there's a leading num_layers dim\n",
    "        d = {}\n",
    "        d.update(unflatten_linear_layers(apply_prefix(prefix, \"c_attn\"), state_dict, self.attn, None))\n",
    "        d.update(unflatten_linear_layers(apply_prefix(prefix, \"c_proj\"), state_dict, self.proj, None))\n",
    "\n",
    "        return super().from_state_dict(d, prefix)\n",
    "\n",
    "    # def from_state_dict(self, state_dict, prefix: str):\n",
    "    #     unflattened_attn = unflatten_linear_layers(\n",
    "    #         prefix,#apply_prefix(prefix, 'c_attn'), \n",
    "    #         state_dict, \n",
    "    #         self.attn, \n",
    "    #         None\n",
    "    #     )\n",
    "        \n",
    "    #     unflattened_proj = unflatten_linear_layers(\n",
    "    #         prefix,\n",
    "    #         #apply_prefix(prefix, 'c_proj'), \n",
    "    #         state_dict, \n",
    "    #         self.proj, \n",
    "    #         None\n",
    "    #     )\n",
    "\n",
    "    #     unflattened_params = {}#{**unflattened_attn, **unflattened_proj}\n",
    "    #     unflattened_params.update(unflattened_attn)\n",
    "    #     unflattened_params.update(unflattened_proj)\n",
    "    #     return super().from_state_dict(unflattened_params, prefix) # extract PyTree from unflattened params\n",
    "\n",
    "    # def update_state_dict(self, state_dict, prefix: str):\n",
    "    #     new_dict={}\n",
    "    #     super().update_state_dict(new_dict, prefix) # write all model params into state_dict\n",
    "\n",
    "    #     flattened_attn = flatten_linear_layers(apply_prefix(prefix, \"c_attn\"), self.attn, None)\n",
    "    #     flattened_proj = flatten_linear_layers(apply_prefix(prefix, \"c_proj\"), self.proj, None)\n",
    "    #     new_dict.update(flattened_attn)\n",
    "    #     new_dict.update(flattened_proj)\n",
    "    #     #new_dict.update({**flattened_attn, **flattened_proj}) # then update with flattened versions \n",
    "    #     state_dict.update(new_dict)\n",
    "    #     return state_dict# | new_dict\n",
    "\n",
    "    def update_state_dict(self, state_dict: StateDict, prefix: Optional[str] = None) -> StateDict:\n",
    "        # need to undo the reshape we did in from_state_dict\n",
    "        # reminder that everything is vectorized\n",
    "        my_dict: StateDict = {}\n",
    "        super().update_state_dict(my_dict, prefix)\n",
    "\n",
    "        my_dict.update(flatten_linear_layers(apply_prefix(prefix, \"c_attn\"), self.attn, None))\n",
    "        my_dict.update(flatten_linear_layers(apply_prefix(prefix, \"c_proj\"), self.proj, None))\n",
    "\n",
    "        state_dict.update(my_dict)\n",
    "        return state_dict\n",
    "\n",
    "\n",
    "\n",
    "class MLP(eqx.Module, Serializable):\n",
    "    proj_up  : hnn.Linear\n",
    "    proj_down: hnn.Linear\n",
    "    dropout  : hnn.Dropout\n",
    "\n",
    "    @staticmethod\n",
    "    def init(config: GPT2Config, key) -> 'MLP':\n",
    "        k1, k2 = jr.split(key, 2)\n",
    "\n",
    "        proj_up = hnn.Linear.init(\n",
    "            In       = config.Embed, \n",
    "            Out      = config.Mlp,\n",
    "            key      = k1,\n",
    "            use_bias = config.use_bias\n",
    "        )\n",
    "\n",
    "        proj_down = hnn.Linear.init(\n",
    "            In       = config.Mlp,\n",
    "            Out      = config.Embed,\n",
    "            key      = k2,\n",
    "            use_bias = config.use_bias\n",
    "        )\n",
    "\n",
    "        dropout = hnn.Dropout(config.dropout)\n",
    "        return MLP(proj_up, proj_down, dropout)\n",
    "\n",
    "    def __call__(self, \n",
    "        x: NamedArray,\n",
    "        *, \n",
    "        key\n",
    "    ) -> NamedArray:\n",
    "        x = self.proj_up(x)\n",
    "        x = hnn.gelu(x)\n",
    "        x = self.proj_down(x)\n",
    "        x = self.dropout(x, key=key)\n",
    "        return x\n",
    "\n",
    "    def _state_dict_key_map(self):\n",
    "        return {'proj_up' : 'c_fc', 'proj_down' : 'c_proj'}\n",
    "\n",
    "\n",
    "class Block(eqx.Module):\n",
    "    ln_1: hnn.LayerNorm\n",
    "    ln_2: hnn.LayerNorm\n",
    "    attn: CausalSelfAttention\n",
    "    mlp : MLP\n",
    "\n",
    "    @staticmethod\n",
    "    def init(config: GPT2Config, key) -> 'Block':\n",
    "        k1, k2 = jr.split(key, 2)\n",
    "\n",
    "        ln_1 = hnn.LayerNorm.init(config.Embed, use_bias=config.use_bias)\n",
    "        ln_2 = hnn.LayerNorm.init(config.Embed, use_bias=config.use_bias)\n",
    "        attn = CausalSelfAttention.init(config, key=k1)\n",
    "        mlp  = MLP.init(config, key=k2)\n",
    "\n",
    "        return Block(ln_1, ln_2, attn, mlp)\n",
    "\n",
    "    def __call__(\n",
    "        self, \n",
    "        x: NamedArray, \n",
    "        padding_mask: NamedArray = None,\n",
    "        *,\n",
    "        key\n",
    "    ) -> NamedArray:\n",
    "        k1, k2 = jr.split(key, 2)\n",
    "\n",
    "        x      = self.ln_1(x)\n",
    "        attn_x = self.attn(x, padding_mask, key=k1)\n",
    "        x      = x + attn_x # residual connection\n",
    "\n",
    "        x      = self.ln_2(x)\n",
    "        ff_x   = self.mlp(x, key=k2)\n",
    "        x      = x + ff_x # residual connection\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT2(eqx.Module, Serializable):\n",
    "    tok_embedding_table: hax.NamedArray\n",
    "    pos_embedding_table: hax.NamedArray\n",
    "    dropout            : hnn.Dropout\n",
    "    ln_f               : hnn.LayerNorm\n",
    "    blocks             : hnn.Stacked\n",
    "    config             : GPT2Config = eqx.static_field()\n",
    "    inference          : bool       #= eqx.static_field()\n",
    "\n",
    "    @staticmethod\n",
    "    def init(config: GPT2Config, key) -> 'GPT2':\n",
    "        k1, k2, k3, k4 = jr.split(key, 4)\n",
    "\n",
    "        tok_embedding_table = hnn.Embedding.init(config.Vocab, config.Embed, key=k1)\n",
    "        pos_embedding_table = hnn.Embedding.init(config.Pos, config.Embed, key=k2)\n",
    "\n",
    "        dropout = hnn.Dropout(config.dropout)\n",
    "        ln_f    = hnn.LayerNorm.init(config.Embed, use_bias=config.use_bias)\n",
    "        blocks  = hnn.Stacked.init(config.Layers, Block)(\n",
    "            config = config,\n",
    "            key    = jr.split(k3, config.Layers.size)\n",
    "        )\n",
    "\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        scale_f    = 1 / jnp.sqrt(2 * config.Layers.size)\n",
    "        get_params = lambda b: [b.stacked.attn.proj.weight, b.stacked.mlp.proj_down.weight]\n",
    "        scaled     = [scale_f * i for i in get_params(blocks)]\n",
    "        blocks     = eqx.tree_at(get_params, blocks, scaled)\n",
    "\n",
    "        return GPT2(\n",
    "            tok_embedding_table, \n",
    "            pos_embedding_table, \n",
    "            dropout,  \n",
    "            ln_f,\n",
    "            blocks,\n",
    "            inference=False,\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self, \n",
    "        seq: hax.NamedArray, \n",
    "        *, \n",
    "        key\n",
    "    ) -> hax.NamedArray:\n",
    "\n",
    "        # ==============================\n",
    "        # input sequence will always be padded to a length of Pos, e.g., during \n",
    "        # inference, so \n",
    "        if self.inference:\n",
    "            seq_padding_mask  = (seq == hax.full(self.config.Pos, -1)).astype(jnp.int32)\n",
    "            attn_padding_mask = seq_padding_mask.broadcast_axis(self.config.KeyPos) \n",
    "        else: attn_padding_mask = None\n",
    "\n",
    "        tok_embs = self.tok_embedding_table.embed(seq) \n",
    "        pos_embs = self.pos_embedding_table.embed(hax.arange(self.config.Pos))\n",
    "        x        = self.dropout(tok_embs + pos_embs, key=key)\n",
    "\n",
    "        x = self.blocks.fold(\n",
    "            x, \n",
    "            None, #attn_padding_mask,\n",
    "            key=jr.split(key, self.config.Layers.size)\n",
    "        )\n",
    "\n",
    "        x      = self.ln_f(x)\n",
    "        logits = self.tok_embedding_table.unembed(x)\n",
    "        return logits\n",
    "\n",
    "    def count_params(self, non_embedding: bool = True):\n",
    "        params = eqx.filter([ # to exclude boolean inference parameter to dropout and layernorm\n",
    "            self.tok_embedding_table, \n",
    "            self.pos_embedding_table if non_embedding else None, \n",
    "            self.dropout, \n",
    "            self.ln_f, \n",
    "            self.blocks\n",
    "        ], eqx.is_inexact_array_like) \n",
    "\n",
    "        leaves, _   = tree_flatten(params)\n",
    "        param_count = sum([i.size for i in leaves])\n",
    "        return param_count\n",
    "\n",
    "    def _pad(self, seq: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Left-pad the array with -1s to maximum context length\"\"\"\n",
    "        padded_array = jnp.pad(seq, (self.config.Pos.size - len(seq), 0), constant_values=-1)\n",
    "        return padded_array\n",
    "        \n",
    "    def generate(\n",
    "        self,\n",
    "        seq: jnp.ndarray,\n",
    "        max_new_tokens: int,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: Optional[int] = None, \n",
    "        *,\n",
    "        key\n",
    "    ) -> jnp.ndarray:\n",
    "        for _ in range(max_new_tokens):\n",
    "            Pos, Vocab = self.config.Pos, self.config.Vocab\n",
    "\n",
    "            seq          = hax.named(self._pad(seq[-Pos.size:]), (Pos,))\n",
    "            logits       = self(seq, key=key)\n",
    "            final_logits = logits[Pos, -1, Vocab, :] / temperature # look at prediction from last token\n",
    "\n",
    "            if top_k is not None:\n",
    "                final_logits = hax.top_k(final_logits, axis=Vocab, k=top_k)\n",
    "\n",
    "            key, subkey  = jr.split(key)\n",
    "            next_token   = hax.random.categorical(logits=final_logits, axis=Vocab, key=subkey)\n",
    "            next_token   = jnp.expand_dims(next_token.array, axis=0) # reshape next_token to [next_token]\n",
    "            seq          = jnp.concatenate([seq, next_token])\n",
    "\n",
    "        return seq\n",
    "\n",
    "    def estimate_mfu(self, fwdbwd_per_iter: int, dt: float) -> float:\n",
    "        \"\"\"Compute model's flop utilization rate \"\"\"\n",
    "        N = self.count_params()\n",
    "        L, H, Q, T = (\n",
    "            self.config.num_layers, \n",
    "            self.config.num_heads,\n",
    "            self.config.hidden_dims // self.config.num_heads,\n",
    "            self.config.seq_len\n",
    "        )\n",
    "\n",
    "        flops_per_token  = 6*N + 12*L*H*Q*T # flops needed to compute one token\n",
    "        flops_per_fwdbwd = flops_per_token * T # flops needed to compute one sequence\n",
    "        flops_per_iter   = flops_per_fwdbwd * fwdbwd_per_iter # flops needed to compute one iteration\n",
    "\n",
    "        flops_achieved   = flops_per_iter * (1.0/dt) # per second\n",
    "        flops_promised   = 312e12 # A100 GPU with bfloat16 has peak of 312 TFLOPS\n",
    "        mfu              = flops_achieved / flops_promised\n",
    "\n",
    "        return mfu\n",
    "\n",
    "    def _state_dict_key_map(self):\n",
    "        return {\n",
    "            \"blocks\"              : \"h\",\n",
    "            \"tok_embedding_table\" : \"wte\", \n",
    "            \"pos_embedding_table\" : \"wpe\"\n",
    "        }\n",
    "\n",
    "    def from_state_dict(self, state_dict, prefix: str):\n",
    "        stacked_params   = stack_state_dict(state_dict, prefix=apply_prefix(prefix, \"h\"))\n",
    "        new_params       = super().from_state_dict(stacked_params, prefix=prefix) # extract PyTree from stacked params\n",
    "        return new_params\n",
    "\n",
    "    def update_state_dict(self, state_dict: StateDict, prefix: Optional[str] = None) -> StateDict:\n",
    "        # this method needs to \"devectorize\" the blocks, so that we have a list of blocks h.0.FOO, h.1.FOO, etc.\n",
    "        # first just do the normal thing with our own dict, which we'll post-process\n",
    "        my_state_dict: StateDict = {}\n",
    "        super().update_state_dict(my_state_dict, prefix)\n",
    "\n",
    "        stacked_dict = unstack_state_dict(my_state_dict, apply_prefix(prefix, \"h\"))\n",
    "        state_dict.update(stacked_dict)\n",
    "\n",
    "        return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sd.keys() # save config in future\n",
    "config = GPT2Config(\n",
    "    seq_len     = 64,\n",
    "    num_layers  = 4,\n",
    "    num_heads   = 4,\n",
    "    hidden_dims = 128,\n",
    "    vocab_size  = 65,\n",
    "    dropout     = 0.0\n",
    ")\n",
    "raw_model = GPT2.init(config, key=jax.random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = raw_model.from_state_dict(model, prefix=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Block' has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPRNGKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 371\u001b[0m, in \u001b[0;36mGPT2.generate\u001b[0;34m(self, seq, max_new_tokens, temperature, top_k, key)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[1;32m    369\u001b[0m     Pos, Vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mPos, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mVocab\n\u001b[0;32m--> 371\u001b[0m     seq          \u001b[38;5;241m=\u001b[39m hax\u001b[38;5;241m.\u001b[39mnamed(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mPos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, (Pos,))\n\u001b[1;32m    372\u001b[0m     logits       \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(seq, key\u001b[38;5;241m=\u001b[39mkey)\n\u001b[1;32m    373\u001b[0m     final_logits \u001b[38;5;241m=\u001b[39m logits[Pos, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, Vocab, :] \u001b[38;5;241m/\u001b[39m temperature \u001b[38;5;66;03m# look at prediction from last token\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 356\u001b[0m, in \u001b[0;36mGPT2._pad\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pad\u001b[39m(\u001b[38;5;28mself\u001b[39m, seq: jnp\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m jnp\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    355\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Left-pad the array with -1s to maximum context length\"\"\"\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m     padded_array \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mpad(seq, (\u001b[43mBlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(seq), \u001b[38;5;241m0\u001b[39m), constant_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m padded_array\n",
      "File \u001b[0;32m~/anaconda3/envs/jax/lib/python3.11/site-packages/equinox/_module.py:486\u001b[0m, in \u001b[0;36m_ModuleMeta.__getattribute__\u001b[0;34m(cls, item)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, item):\n\u001b[0;32m--> 486\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    488\u001b[0m         item \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__wrapped__\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    489\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mproperty\u001b[39m)\n\u001b[1;32m    490\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _has_dataclass_init\n\u001b[1;32m    491\u001b[0m     ):\n\u001b[1;32m    492\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Block' has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "\n",
    "loaded_model.generate(\n",
    "        seq=jnp.array([0]),\n",
    "        max_new_tokens=10,\n",
    "        temperature=1.0,\n",
    "        key=jax.random.PRNGKey(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Block' has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 356\u001b[0m, in \u001b[0;36mGPT2._pad\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pad\u001b[39m(\u001b[38;5;28mself\u001b[39m, seq: jnp\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m jnp\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    355\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Left-pad the array with -1s to maximum context length\"\"\"\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m     padded_array \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mpad(seq, (\u001b[43mBlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(seq), \u001b[38;5;241m0\u001b[39m), constant_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m padded_array\n",
      "File \u001b[0;32m~/anaconda3/envs/jax/lib/python3.11/site-packages/equinox/_module.py:486\u001b[0m, in \u001b[0;36m_ModuleMeta.__getattribute__\u001b[0;34m(cls, item)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, item):\n\u001b[0;32m--> 486\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    488\u001b[0m         item \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__wrapped__\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    489\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mproperty\u001b[39m)\n\u001b[1;32m    490\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _has_dataclass_init\n\u001b[1;32m    491\u001b[0m     ):\n\u001b[1;32m    492\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Block' has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "loaded_model._pad(jnp.array([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
